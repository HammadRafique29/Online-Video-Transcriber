 Hello everyone, welcome to the video how to test the API for the first time. Let's begin. In a previous video we selected the test and techniques to use on our project. Explore your testing and checklist-based testing. Insert the first API checklist and we were informed that we wouldn't receive access to swagger this week. Let's start with exploratory testing today. According to the STGB glossary, the exploratory testing is an approach to testing whereby the testers in amgly design and execute tests based on their knowledge, exploration of the test item and the results of previous tests. You can get familiar with it more on the ISDGB website, but we will simplify this. By simplifying, I mean something similar but not exactly the same. In my opinion, explore testing of the API is the process when you design and execute tests based on theoretical knowledge of the API testing based on the knowledge of the project. When we say design and execute tests, it means that we are going to write test summaries into the Google sheet and inputs outputs, our test data, which we are going to store in the postman. The second item of the exploratory testing is based on the theoretical knowledge of API testing and we had a lot of long videos with chemists that explained the basics of the API testing theory and the last one. Based on the knowledge of the project, as we said, we are new to the project. We don't have any knowledge about it and the documentation is terrible. This conditions are pretty bad, but we are the experts. So let's try to test now. And we start by checking our documentation and it is the same old story as in the previous videos, screenshot of the request, screenshot of the response and the C URL. Let's check the screenshots. On the first screenshot, we can see the request details, the method, the endpoint. We can see that endpoint includes the path parameter, path ID and information about the parameter. Also, we can see the C URL of the request. It looks promising we can run a couple of tests. Let's check the next screenshot before we start. On the second screenshot, we can see the HTTP response documentation. We can see the response code and the response body, which were returned after the call, which is specified in the C URL. And we can see the documentation. Three response codes, 200, 400, 404, and the expected response body for the 200 response code. To be honest, the situation is okay. Could be better, but could be worse. We are the experts, it is easy for us. So from where do we start? When you test APIs, the first thing which you need to do is to send the request and receive the response. And we have the C URL in the story. We will be able to use it to send the first request. Let's copy the C URL. And use the postman import feature as we have done in previous videos. I will skip steps here because it takes a lot of time. Once the request is imported, you can check the request data on the postman screen. The method get and the URL is displayed in the address bar. That is all that we need to send the request. Let's click the send button and check what we receive in the response. And we got the response. The response code is 200. And we can see the response body with a lot of data. Let's compare it to the documentation on the image. And we can see that in the documentation the response code 200 is specified. And we got exactly the same in the postman 200 okay. But when we check the response body on the screenshot, we can see a small body over cat and the cat's name is Tom. And we got completely different data. What do you think about this? Have we found our first bug? But we haven't started testing yet. Now, I don't think that is a bug. We are the experts and we know a simple rule. If you want to get some specific data, you must create the data before the test. And we don't know how to set the data. It will be another obstacle on our path. At least we got the 200 response code. And we see the data structure looks like a valid one. We were able to send a valid request and receive a valid response. Let's say the request in the postman. And we have walked through Saveflow twice already. So I will skip it. And we have saved it in a very organized manner. We have the collection, PathStoreSwagger. It is the domain name of the service. We can see it in the URL. After that, we have the folder v2.path.pathID. This is our endpoint. Can be checked in the URL as well. Instead of the number 1, I put the path ID path parameter as was on the screenshots of the swagger. And then the name of the test case, the happy path. And I hope that all of you know what means the happy path test case. It's when you pass the test case and you're happy. Not exactly, but you got the point. Okay, what I really mean by the happy path is the positive test. Some people call it a smoke test or a functional test. So the test cases cover positive scenarios. Check if the main functionality of a feature works properly. Check a feature in a way it is intended to be used. Check if the feature meets the functional requirements. And we were able to partially run this test case. It is the time to create our checklist. And we will start with the checklist structure. The same as in the postman. We will have a domain name and the endpoint. And then two test cases. The happy path test and negative test case. Why do we need two test cases? On the projects where people do not invest in processes, documentation and releases almost every day. It is a good idea to start with at least two tests per endpoint. Mostly it will be enough. Mostly you won't be able to increase coverage because you won't have enough time. We will talk about this in the future videos. For now, let's finish with our two test cases. The dropped version of the happy path test case is already created in postman. Let's do the same for the negative test. But before that, let's do a small coffee break. Running the first happy path mostly takes a lot of time. After this, everything goes much faster. Let's run the negative test now. You can have the question. Who said that the negative test case is needed at all? We have bad requirements and the functionality is not learned yet. This is the bad work environment. Why do we try to test it? The answer to all those questions. I am the one who said this. I created the video and we need to do it in this way. It is a joke, of course. Come down. The real answer is that it depends from a lot of things. A project, functionality, people, team maturity, time and processes. And only you are the one who can say if you need 1000 test cases or only 2 test cases. Or no test cases at all. My recommendation is, no matter how bad the project is, how bad the accommodation is, you need to have at least 2 test cases. Positive, happy path and negative break the functionality. If you have those, it means you understand and list the basics of the endpoint. So what is the negative test case? The official definition says, negative testing, testing a component or system in a way for which it was not intended to be used. I just shared the definition. We need to learn this together. But if you will ask me, my definition of negative testing is, you know what to do. All we know is what to do. We are testers. We are the experts. We know how to break things. But let's check the documentation first. When we check the request screen, we can see that we have proper documentation of the path parameter at I.B. The one at the end of the URL. It says integer int64. And also you can see the red asterix. It means that the parameter is required. And now we really know what to do. Let's break it. We can send a text instead of a number. In this case, we get 404 and job performance exception error. We can send a large number. In this case, we get the 404 and job performance exception error as well. We can send a very large number. In this case, we get the same result as previous one. We can delete a path ID parameter. In this case, we get the 405 error. We can send the post method instead of get request. In this case, we get the 200 and no clear error. We can send even the SQL or JavaScript in the path. In this case, we get the 404 and job performance exception error. We create a lot of scenarios like this. And it looks like we know what to do when do negative testing. But let's ask ourselves the question. Do we really know what we are doing? What would experts do? What is the purpose of all of this? First of all, as we said before. For now, for the first iteration of the explorer testing, it is enough to save at least one negative test. We are testers and the tests make our job visible. Test cases are very important artifacts. Because of this, let's run and save only one negative test for now. But there is another question to answer. How do select only one negative scenario? What would expert do? My answer is that we need to predict user behavior and try the most common mistake the end user can make. Again, for now, you can't be sure. You don't know the project and you don't have proper documentation. So it is your choice. And my choice is that the user won't specify the pad ID. So I saved another get request. In the get slash v2 slash pad slash pad ID folder, negative test case. And one more thing which we need to do is to save the response. It will help us in the future and it will be our expert result. We need to expand the save response drop down on the right of the screen. And then select the save as example option. After that, we can see the response saved under the rest. So in the future, you'll be able to check what kind of response you got there. And I want to emphasize one thing. I show not how it should be. I show how I would do this. All of this is just recommendations. Postman is the tool and there are different ways to use it. For example, you could not save the get one more time. You could do two under the same request. Or your collections and folders structure can be different. Find your own way. What will work for you? There is one more thing I need to mention. I made a couple of mistakes in previous slides. First of all, I forgot to put .io in the collection name. I have fixed it. Second, I forgot to put .gat into the folder name. I have fixed it. And I forgot to save the example of the positive test case response. Again, I have fixed it. Let's check the results in the Google sheet. So, we have two test cases written in our checklist in Google sheet. And we have two test cases written in the postman. Is it enough? Can we go drink coffee and do nothing? As I said, for the first iteration of the exposure testing, it is enough to have two test cases. And the first iteration is over. Now it is time for the next iteration. We know what the happy path looks like. And we know the negative test as well. It is time to learn more about the endpoint. Let's read the documentation one more time. And we are going to do it in the next video.